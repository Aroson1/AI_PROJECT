{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d6d4df",
   "metadata": {},
   "source": [
    "# Fast Neural Style Transfer - CSE 311 AI Project\n",
    "\n",
    "**Project Title:** Real-Time Artistic Image Stylization Using Deep Neural Networks\n",
    "\n",
    "This notebook demonstrates training and inference for Fast Neural Style Transfer on Google Colab.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Neural Style Transfer allows us to apply the artistic style of one image to the content of another. This implementation uses:\n",
    "\n",
    "- **Generator Network**: Feed-forward CNN with residual blocks (TransformerNet)\n",
    "- **Loss Function**: VGG19-based content, style, and total variation losses\n",
    "- **Dataset**: COCO 2017 (subset of ~2,000 images at 256×256)\n",
    "- **Training**: 2-3 epochs optimized for Colab Free Tier\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda40e95",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's clone the repository and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b73886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your repo URL)\n",
    "!git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git\n",
    "%cd YOUR_REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dbd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install minimal dependencies\n",
    "!pip install -q torch torchvision tqdm matplotlib Pillow numpy opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90023a",
   "metadata": {},
   "source": [
    "## 2. Download Dataset and Style Image\n",
    "\n",
    "Download a subset of COCO 2017 dataset and a style image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download COCO 2017 validation set (smaller than train set)\n",
    "!wget -q http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip -q val2017.zip\n",
    "print(\"COCO dataset downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10806272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use existing style image from repo or download one\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the style image\n",
    "style_image_path = \"picasso_selfportrait.jpg\"  # Or your chosen style image\n",
    "\n",
    "if os.path.exists(style_image_path):\n",
    "    style_img = Image.open(style_image_path)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(style_img)\n",
    "    plt.title(\"Style Image\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Style image not found at {style_image_path}\")\n",
    "    print(\"Please upload a style image or update the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6265b",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Set up training parameters optimized for Colab Free Tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    \"dataset_path\": \"val2017\",  # Path to COCO images\n",
    "    \"style_image\": \"picasso_selfportrait.jpg\",  # Style image path\n",
    "    \"epochs\": 3,  # Number of training epochs\n",
    "    \"batch_size\": 4,  # Batch size (adjust based on GPU memory)\n",
    "    \"image_size\": 256,  # Image resolution\n",
    "    \"lr\": 1e-3,  # Learning rate\n",
    "    \"content_weight\": 1.0,  # Content loss weight\n",
    "    \"style_weight\": 10.0,  # Style loss weight\n",
    "    \"tv_weight\": 1e-6,  # Total variation loss weight\n",
    "    \"subset_size\": 2000,  # Use 2000 images from COCO\n",
    "    \"checkpoint_dir\": \"models/checkpoints\",\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25de23",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n",
    "\n",
    "Train the Fast Neural Style Transfer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f993e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformer import TransformerNet\n",
    "from vgg_loss import StyleTransferLoss\n",
    "from datasets import create_dataloader, get_style_image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}\\n\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing transformer network...\")\n",
    "transformer = TransformerNet().to(device)\n",
    "num_params = sum(p.numel() for p in transformer.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\\n\")\n",
    "\n",
    "# Initialize loss function\n",
    "print(\"Initializing loss function with VGG19...\")\n",
    "loss_fn = StyleTransferLoss(\n",
    "    content_weight=config[\"content_weight\"],\n",
    "    style_weight=config[\"style_weight\"],\n",
    "    tv_weight=config[\"tv_weight\"],\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=config[\"lr\"])\n",
    "print(f\"Optimizer: Adam (lr={config['lr']})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366039f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and pre-process style image\n",
    "print(f\"Loading style image: {config['style_image']}\")\n",
    "style_transform = get_style_image_transform(config[\"image_size\"])\n",
    "style_image = Image.open(config[\"style_image\"]).convert(\"RGB\")\n",
    "style_tensor = style_transform(style_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Pre-compute style Gram matrices\n",
    "print(\"Computing style features...\")\n",
    "style_grams = loss_fn.extract_style_gram(style_tensor)\n",
    "print(\"Style features computed!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "print(f\"Loading dataset from: {config['dataset_path']}\")\n",
    "dataloader = create_dataloader(\n",
    "    config[\"dataset_path\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    image_size=config[\"image_size\"],\n",
    "    subset_size=config[\"subset_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "print(f\"Dataloader ready: {len(dataloader)} batches per epoch\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9567cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting Training: {config['epochs']} epochs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "loss_history = {\"total\": [], \"content\": [], \"style\": [], \"tv\": []}\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    transformer.train()\n",
    "    epoch_losses = {\"total\": 0, \"content\": 0, \"style\": 0, \"tv\": 0}\n",
    "    num_batches = 0\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # Progress bar\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "    for batch_idx, content_batch in enumerate(pbar):\n",
    "        content_batch = content_batch.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        generated_batch = transformer(content_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        losses = loss_fn(\n",
    "            generated_batch, content_batch, style_features_gram=style_grams\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        losses[\"total\"].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        epoch_losses[\"total\"] += losses[\"total\"].item()\n",
    "        epoch_losses[\"content\"] += losses[\"content\"]\n",
    "        epoch_losses[\"style\"] += losses[\"style\"]\n",
    "        epoch_losses[\"tv\"] += losses[\"tv\"]\n",
    "        num_batches += 1\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"loss\": f\"{losses['total'].item():.2f}\",\n",
    "                \"content\": f\"{losses['content']:.2f}\",\n",
    "                \"style\": f\"{losses['style']:.2f}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}\n",
    "\n",
    "    # Store losses\n",
    "    for key in loss_history:\n",
    "        loss_history[key].append(avg_losses[key])\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['epochs']} - Time: {epoch_time:.2f}s\")\n",
    "    print(f\"  Avg Total Loss:   {avg_losses['total']:.4f}\")\n",
    "    print(f\"  Avg Content Loss: {avg_losses['content']:.4f}\")\n",
    "    print(f\"  Avg Style Loss:   {avg_losses['style']:.4f}\")\n",
    "    print(f\"  Avg TV Loss:      {avg_losses['tv']:.6f}\\n\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(\n",
    "        config[\"checkpoint_dir\"], f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "    )\n",
    "    torch.save(transformer.state_dict(), checkpoint_path)\n",
    "    print(f\"✓ Checkpoint saved: {checkpoint_path}\\n\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(config[\"checkpoint_dir\"], \"final_model.pth\")\n",
    "torch.save(transformer.state_dict(), final_model_path)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Final model saved: {final_model_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4df10",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Progress\n",
    "\n",
    "Plot the loss curves to see how the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fe7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Total loss\n",
    "axes[0, 0].plot(loss_history[\"total\"], \"b-\", linewidth=2, marker=\"o\")\n",
    "axes[0, 0].set_title(\"Total Loss\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Content loss\n",
    "axes[0, 1].plot(loss_history[\"content\"], \"g-\", linewidth=2, marker=\"o\")\n",
    "axes[0, 1].set_title(\"Content Loss\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Loss\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Style loss\n",
    "axes[1, 0].plot(loss_history[\"style\"], \"r-\", linewidth=2, marker=\"o\")\n",
    "axes[1, 0].set_title(\"Style Loss\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_ylabel(\"Loss\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# TV loss\n",
    "axes[1, 1].plot(loss_history[\"tv\"], \"m-\", linewidth=2, marker=\"o\")\n",
    "axes[1, 1].set_title(\"Total Variation Loss\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1, 1].set_xlabel(\"Epoch\")\n",
    "axes[1, 1].set_ylabel(\"Loss\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "loss_plot_path = os.path.join(config[\"checkpoint_dir\"], \"loss_plot.png\")\n",
    "plt.savefig(loss_plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"Loss plot saved: {loss_plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd06fe65",
   "metadata": {},
   "source": [
    "## 6. Inference - Stylize Test Images\n",
    "\n",
    "Use the trained model to stylize new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e135d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "from inference import load_model, stylize_image\n",
    "from datasets import tensor_to_image\n",
    "\n",
    "# Load model\n",
    "model = load_model(final_model_path, device=device)\n",
    "print(\"Model loaded for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f0cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample image\n",
    "test_image_path = \"japanese_garden.jpg\"  # Or upload your own\n",
    "\n",
    "if os.path.exists(test_image_path):\n",
    "    output = stylize_image(\n",
    "        model,\n",
    "        test_image_path,\n",
    "        output_path=\"stylized_output.jpg\",\n",
    "        device=device,\n",
    "        display=True,\n",
    "    )\n",
    "else:\n",
    "    print(f\"Test image not found: {test_image_path}\")\n",
    "    print(\"Please upload an image or use an image from COCO dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on multiple sample images from COCO\n",
    "import random\n",
    "from datasets import get_test_image_transform\n",
    "\n",
    "# Get random images from dataset\n",
    "coco_images = [\n",
    "    os.path.join(config[\"dataset_path\"], f)\n",
    "    for f in os.listdir(config[\"dataset_path\"])\n",
    "    if f.endswith(\".jpg\")\n",
    "][\n",
    "    :10\n",
    "]  # Get first 10\n",
    "\n",
    "# Select 3 random images\n",
    "sample_images = random.sample(coco_images, min(3, len(coco_images)))\n",
    "\n",
    "fig, axes = plt.subplots(len(sample_images), 2, figsize=(12, 4 * len(sample_images)))\n",
    "\n",
    "for idx, img_path in enumerate(sample_images):\n",
    "    # Load content image\n",
    "    content_img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # Stylize\n",
    "    output = stylize_image(model, img_path, device=device, display=False)\n",
    "\n",
    "    # Display\n",
    "    if len(sample_images) == 1:\n",
    "        axes[0].imshow(content_img)\n",
    "        axes[0].set_title(\"Original\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(output)\n",
    "        axes[1].set_title(\"Stylized\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[1].axis(\"off\")\n",
    "    else:\n",
    "        axes[idx, 0].imshow(content_img)\n",
    "        axes[idx, 0].set_title(f\"Original {idx+1}\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[idx, 0].axis(\"off\")\n",
    "\n",
    "        axes[idx, 1].imshow(output)\n",
    "        axes[idx, 1].set_title(f\"Stylized {idx+1}\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[idx, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sample_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSample results saved to: sample_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c105b",
   "metadata": {},
   "source": [
    "## 7. Summary and Results\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✓ Training a Fast Neural Style Transfer model on COCO dataset\n",
    "2. ✓ Using VGG19 for content and style loss computation\n",
    "3. ✓ Optimizing for Colab Free Tier (256×256, 2-3 epochs, batch size 4)\n",
    "4. ✓ Visualizing training progress with loss plots\n",
    "5. ✓ Running inference on test images\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- Model successfully learned to apply artistic style to arbitrary images\n",
    "- Content and style losses decreased over epochs\n",
    "- Training time: ~X minutes per epoch on Colab GPU\n",
    "- Inference time: ~X seconds per image\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Experiment with different style images\n",
    "- Adjust loss weights for different stylization effects\n",
    "- Train for more epochs for better quality\n",
    "- Try higher resolution images (512×512)\n",
    "\n",
    "---\n",
    "\n",
    "**CSE 311 Artificial Intelligence Project**  \n",
    "*Real-Time Artistic Image Stylization Using Deep Neural Networks*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
